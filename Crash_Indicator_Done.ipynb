{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crash indicator (concise)\n",
    "\n",
    "Same pipeline as Crash_Indicator.ipynb: short titles and descriptions above each section; no long analysis blocks under the plots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Import libraries and set plot style.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aa9cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from scipy import linalg\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "See code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24744705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S&P 500 stocks with long trading history\n",
    "# Paper uses N=194 stocks from 1985-2016\n",
    "SP500_TICKERS = [\n",
    "    'CMCSA', 'DIS', 'F', 'GPC', 'GT', 'HAS', 'HD', 'HRB', 'IPG',\n",
    "    'LEG', 'LEN', 'LOW', 'MAT', 'MCD', 'NKE', 'SHW', 'TGT',\n",
    "    'VFC', 'WHR', 'ADM', 'CAG', 'CL', 'CPB', 'CVS', 'GIS', 'HRL',\n",
    "    'HSY', 'K', 'KMB', 'KO', 'KR', 'MKC', 'MO', 'SYY', 'TAP', 'TSN',\n",
    "    'WMT', 'APA', 'COP', 'CVX', 'HAL', 'HP', 'MUR',\n",
    "    'NBR', 'SLB', 'VLO', 'WMB', 'XOM', 'AFL', 'AIG', 'AON',\n",
    "    'AXP', 'BAC', 'BBT', 'BEN', 'BK', 'CB', 'CINF', 'CMA', 'C', 'EFX',\n",
    "    'FHN', 'HBAN', 'HST', 'JPM', 'L', 'LNC', 'MMC',\n",
    "    'MTB', 'PSA', 'SLM', 'TRV', 'USB', 'VNO', 'WFC', 'WY', 'ZION',\n",
    "    'ABT', 'AET', 'AMGN', 'BAX', 'BDX', 'BMY', 'CAH', 'CI', 'HUM',\n",
    "    'JNJ', 'LLY', 'MDT', 'MRK', 'SYK', 'THC', 'TMO', 'UNH',\n",
    "    'AVY', 'BA', 'CAT', 'CMI', 'CSX', 'CTAS', 'DE', 'DHR', 'DOV',\n",
    "    'EMR', 'ETN', 'EXPD', 'FDX', 'FLS', 'GD', 'GE', 'GLW', 'GWW', 'HON',\n",
    "    'ITW', 'LMT', 'LUV', 'MAS', 'MMM', 'ROK', 'TXT',\n",
    "    'UNP', 'AAPL', 'ADI', 'ADP', 'AMAT', 'AMD', 'HPQ',\n",
    "    'IBM', 'INTC', 'KLAC', 'LRCX', 'MSI', 'MU', 'TXN', 'WDC', 'XRX',\n",
    "    'AA', 'APD', 'BMS', 'CLF', 'DD', 'ECL', 'FMC', 'IFF', 'IP',\n",
    "    'NEM', 'PPG', 'VMC', 'T', 'VZ', 'AEP', 'CMS', 'CNP',\n",
    "    'D', 'DTE', 'ED', 'EIX', 'EQT', 'ETR', 'EXC', 'NEE', 'NI',\n",
    "    'PNW', 'SO', 'WEC', 'XEL'\n",
    "]\n",
    "\n",
    "# Parameters from paper: M=40 days, Δ=20 days shift\n",
    "START_DATE = '1985-01-02'  # Adjust based on data availability\n",
    "END_DATE = '2024-12-31'\n",
    "EPOCH_SIZE = 4  # M = 40 days\n",
    "SHIFT = 2       # Δ = 20 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data download\n",
    "\n",
    "Fetch adjusted close prices from yfinance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee348ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(tickers, start_date, end_date):\n",
    "    print(f\"Fetching {len(tickers)} stocks from {start_date} to {end_date}...\")\n",
    "    \n",
    "    data = yf.download(\n",
    "        tickers,\n",
    "        start=start_date,\n",
    "        end=end_date,\n",
    "        progress=True,\n",
    "        auto_adjust=True\n",
    "    )['Close']\n",
    "    \n",
    "    data = data.ffill().bfill()\n",
    "    data = data.dropna(axis=1)\n",
    "    data = data.dropna(axis=0)\n",
    "\n",
    "    print(f\"Final: {data.shape[0]} trading days, {data.shape[1]} stocks\")\n",
    "    return data\n",
    "\n",
    "# Fetch data\n",
    "prices = fetch_data(SP500_TICKERS, START_DATE, END_DATE)\n",
    "N_STOCKS = prices.shape[1]\n",
    "print(f\"\\nN = {N_STOCKS} stocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core metrics\n",
    "\n",
    "Log returns, correlation matrix, eigen-centrality, entropy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624b0b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_returns(prices):\n",
    "    \"\"\"Compute log returns: r_i(τ) = ln P_i(τ) - ln P_i(τ-1)\"\"\"\n",
    "    return np.log(prices / prices.shift(1)).dropna()\n",
    "\n",
    "def compute_correlation_matrix(returns):\n",
    "    \"\"\"Compute Pearson correlation matrix C with NaN handling.\"\"\"\n",
    "    C = np.corrcoef(returns.T)\n",
    "    C = np.nan_to_num(C, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "    C = (C + C.T) / 2\n",
    "    np.fill_diagonal(C, 1.0)\n",
    "    \n",
    "    return C\n",
    "\n",
    "def correlation_to_A_matrix(C):\n",
    "    \"\"\"\n",
    "    Convert correlation matrix C to matrix A = |C|^2.\n",
    "    This ensures non-negative entries for Perron-Frobenius theorem.\n",
    "    Paper uses n=2 (squared).\n",
    "    \"\"\"\n",
    "    return np.abs(C) ** 2\n",
    "\n",
    "def compute_eigen_centrality(A):\n",
    "    \"\"\"\n",
    "    Compute eigen-centrality from matrix A.\n",
    "    Returns normalized eigenvector of largest eigenvalue.\n",
    "    p_i >= 0 and sum(p_i) = 1\n",
    "    \"\"\"\n",
    "    eigenvalues, eigenvectors = linalg.eigh(A)\n",
    "    # Get eigenvector for largest eigenvalue\n",
    "    idx_max = np.argmax(eigenvalues)\n",
    "    v = eigenvectors[:, idx_max]\n",
    "    # Ensure non-negative (by Perron-Frobenius, should be)\n",
    "    v = np.abs(v)\n",
    "    # Normalize so sum = 1\n",
    "    p = v / np.sum(v)\n",
    "    return p\n",
    "\n",
    "def compute_entropy(p):\n",
    "    \"\"\"\n",
    "    Compute Shannon entropy: H = -sum(p_i * ln(p_i))\n",
    "    \"\"\"\n",
    "    p_nonzero = p[p > 1e-12]\n",
    "    return -np.sum(p_nonzero * np.log(p_nonzero))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "See code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd5f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_correlation_matrix(C):\n",
    "    \"\"\"\n",
    "    Decompose C into market mode C_M and group-random mode C_GR.\n",
    "    \n",
    "    C = C_M + C_GR\n",
    "    C_M = λ₁|e₁⟩⟨e₁|  (rank-1 matrix from largest eigenvalue)\n",
    "    C_GR = Σᵢ₌₂ᴺ λᵢ|eᵢ⟩⟨eᵢ|  (all other eigenvalues)\n",
    "    \"\"\"\n",
    "    C = np.nan_to_num(C, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "    C = (C + C.T) / 2\n",
    "    np.fill_diagonal(C, 1.0)\n",
    "    eigenvalues, eigenvectors = linalg.eigh(C)\n",
    "    \n",
    "    # Sort by eigenvalue (descending)\n",
    "    idx = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "    \n",
    "    N = len(eigenvalues)\n",
    "    \n",
    "    # Market mode: C_M = λ₁ * |e₁⟩⟨e₁|\n",
    "    e1 = eigenvectors[:, 0].reshape(-1, 1)\n",
    "    C_M = eigenvalues[0] * (e1 @ e1.T)\n",
    "    \n",
    "    # Group-Random mode: C_GR = sum over i=2 to N\n",
    "    C_GR = np.zeros_like(C)\n",
    "    for i in range(1, N):\n",
    "        ei = eigenvectors[:, i].reshape(-1, 1)\n",
    "        C_GR += eigenvalues[i] * (ei @ ei.T)\n",
    "    \n",
    "    return C_M, C_GR, eigenvalues, eigenvectors\n",
    "\n",
    "def compute_three_entropies(C):\n",
    "    \"\"\"\n",
    "    Compute all three entropies as in the paper:\n",
    "    - H: from full correlation matrix C\n",
    "    - H_M: from market mode C_M\n",
    "    - H_GR: from group-random mode C_GR\n",
    "    \"\"\"\n",
    "    # Decompose C\n",
    "    C_M, C_GR, eigenvalues, eigenvectors = decompose_correlation_matrix(C)\n",
    "    \n",
    "    # Convert to A matrices (squared entries)\n",
    "    A = correlation_to_A_matrix(C)\n",
    "    A_M = correlation_to_A_matrix(C_M)\n",
    "    A_GR = correlation_to_A_matrix(C_GR)\n",
    "    \n",
    "    # Compute eigen-centralities\n",
    "    p = compute_eigen_centrality(A)\n",
    "    p_M = compute_eigen_centrality(A_M)\n",
    "    p_GR = compute_eigen_centrality(A_GR)\n",
    "    \n",
    "    # Compute entropies\n",
    "    H = compute_entropy(p)\n",
    "    H_M = compute_entropy(p_M)\n",
    "    H_GR = compute_entropy(p_GR)\n",
    "    \n",
    "    return H, H_M, H_GR, eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling analysis\n",
    "\n",
    "Rolling-window entropy and correlation time series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a3d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window_analysis(returns_df, epoch_size=40, shift=20):\n",
    "    \"\"\"\n",
    "    Perform rolling window analysis following the paper exactly.\n",
    "    Computes H, H_M, H_GR for each epoch.\n",
    "    \"\"\"\n",
    "    returns = returns_df.values\n",
    "    dates = returns_df.index\n",
    "    N = returns.shape[1]\n",
    "    H_max = np.log(N)  # Wishart limit\n",
    "    \n",
    "    results = []\n",
    "    start = 0\n",
    "    \n",
    "    while start + epoch_size <= len(returns):\n",
    "        epoch_returns = returns[start:start + epoch_size]\n",
    "        epoch_end_date = dates[start + epoch_size - 1]\n",
    "        \n",
    "        # Correlation matrix\n",
    "        C = compute_correlation_matrix(epoch_returns)\n",
    "        \n",
    "        # Three entropies (paper's method)\n",
    "        H, H_M, H_GR, eigenvalues = compute_three_entropies(C)\n",
    "        \n",
    "        # Mean market correlation μ\n",
    "        upper_tri = np.triu_indices(N, k=1)\n",
    "        mu = np.mean(C[upper_tri])\n",
    "        \n",
    "        # Phase space coordinates\n",
    "        H_minus_HM = H - H_M\n",
    "        H_minus_HGR = H - H_GR\n",
    "        HM_minus_HGR = H_M - H_GR\n",
    "        \n",
    "        # Largest eigenvalue\n",
    "        lambda_max = eigenvalues[0]\n",
    "        \n",
    "        results.append({\n",
    "            'date': epoch_end_date,\n",
    "            'H': H,\n",
    "            'H_M': H_M,\n",
    "            'H_GR': H_GR,\n",
    "            'H_max': H_max,\n",
    "            'H_minus_HM': H_minus_HM,\n",
    "            'H_minus_HGR': H_minus_HGR,\n",
    "            'HM_minus_HGR': HM_minus_HGR,\n",
    "            'abs_H_minus_HM': np.abs(H_minus_HM),\n",
    "            'abs_H_minus_HGR': np.abs(H_minus_HGR),\n",
    "            'mu': mu,  # mean correlation\n",
    "            'lambda_max': lambda_max,\n",
    "        })\n",
    "        \n",
    "        start += shift\n",
    "        \n",
    "    return pd.DataFrame(results)\n",
    "    \n",
    "# Compute returns\n",
    "returns = compute_log_returns(prices)\n",
    "results = rolling_window_analysis(returns, epoch_size=EPOCH_SIZE, shift=SHIFT)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre/Post-2005 split\n",
    "\n",
    "Split results and create pre-2005 and post-2005 plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d134804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into pre-2005 and post-2005\n",
    "split_date = pd.Timestamp('2005-01-01')\n",
    "results_pre2005 = results[results['date'] < split_date].copy()\n",
    "results_post2005 = results[results['date'] >= split_date].copy()\n",
    "\n",
    "MARKET_EVENTS = {\n",
    "    'crashes': [\n",
    "        # Pre-2000 events (from paper's Table S3)\n",
    "        ('1987-10-19', '1987-10-30', 'Black Monday'),\n",
    "        ('1989-10-13', '1989-10-20', 'Friday 13th Mini Crash'),\n",
    "        ('1990-01-01', '1990-12-31', 'Early 90s Recession'),\n",
    "        ('1997-10-27', '1997-11-15', 'Asian Financial Crisis'),\n",
    "        # 2000s events\n",
    "        ('2000-03-10', '2000-04-14', 'Dot-com Crash'),\n",
    "        ('2001-09-11', '2001-09-30', '9/11 Financial Crisis'),\n",
    "        ('2002-09-01', '2002-10-15', 'Stock Market Downturn 2002'),\n",
    "        ('2008-09-15', '2008-11-30', 'Lehman Brothers Crash'),\n",
    "        ('2010-05-06', '2010-05-20', 'DJ Flash Crash'),\n",
    "        ('2011-03-11', '2011-03-25', 'Tsunami/Fukushima'),\n",
    "        ('2011-08-08', '2011-08-31', 'August 2011 Markets Fall'),\n",
    "        ('2015-08-24', '2015-09-15', 'Chinese Black Monday'),\n",
    "        ('2018-02-02', '2018-02-09', 'Volatility spike'),\n",
    "        ('2020-02-20', '2020-03-23', 'COVID Crash'),\n",
    "    ],\n",
    "    'bubbles': [\n",
    "        ('1999-01-01', '2000-03-09', 'Dot-com Bubble'),\n",
    "        ('2005-01-01', '2007-10-01', 'US Housing Bubble'),\n",
    "        ('2021-01-01', '2021-11-15', 'Post-COVID Rally'),\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Filter events by date\n",
    "def filter_events_by_date(events_dict, split_date, before=True):\n",
    "    \"\"\"Filter events to only include those before or after split_date\"\"\"\n",
    "    filtered = {'crashes': [], 'bubbles': []}\n",
    "    for event_type in ['crashes', 'bubbles']:\n",
    "        for start, end, name in events_dict[event_type]:\n",
    "            event_start = pd.Timestamp(start)\n",
    "            if (before and event_start < split_date) or (not before and event_start >= split_date):\n",
    "                filtered[event_type].append((start, end, name))\n",
    "    return filtered\n",
    "\n",
    "events_pre2005 = filter_events_by_date(MARKET_EVENTS, split_date, before=True)\n",
    "events_post2005 = filter_events_by_date(MARKET_EVENTS, split_date, before=False)\n",
    "\n",
    "# Crisis periods for shading (split by date)\n",
    "crisis_periods_pre2005 = [\n",
    "    ('1987-10-19', '1987-10-30', 'Black Monday'),\n",
    "    ('1997-10-27', '1997-11-15', 'Asian Crisis'),\n",
    "    ('2000-03-10', '2000-04-14', 'Dot-com Crash'),\n",
    "    ('2001-09-11', '2001-09-30', '9/11'),\n",
    "]\n",
    "\n",
    "crisis_periods_post2005 = [\n",
    "    ('2008-09-15', '2008-11-30', 'Lehman'),\n",
    "    ('2011-08-08', '2011-08-31', 'Aug 2011'),\n",
    "    ('2015-08-24', '2015-09-15', 'China'),\n",
    "]\n",
    "\n",
    "# Function to create plots\n",
    "def create_plots(results_data, events_dict, crisis_periods, title_suffix):\n",
    "    \"\"\"\n",
    "    Create a multi-panel plot visualizing key market indicators and highlight crisis periods.\n",
    "\n",
    "    Parameters:\n",
    "        results_data (pd.DataFrame): DataFrame containing columns:\n",
    "            - 'date': Dates of observations\n",
    "            - 'mu': Mean correlation values\n",
    "            - 'H_minus_HM': Difference between H and H_M indicators\n",
    "            - 'lambda_max': Largest eigenvalue of the correlation matrix\n",
    "        events_dict (dict): Dictionary with keys 'crashes' and 'bubbles', not used in plotting in this function.\n",
    "        crisis_periods (list of tuples): Each tuple: (start_date, end_date, label) of crisis periods to shade on the plot.\n",
    "        title_suffix (str): Text to append to the main plot title.\n",
    "\n",
    "    Creates a 4x1 subplot showing:\n",
    "      - Panel 1: Mean correlation μ\n",
    "      - Panel 2: H - H_M\n",
    "      - Panel 3: -ln(H - H_M) (\"Fear Gauge\" similar to VIX)\n",
    "      - Panel 4: Largest eigenvalue λ_max\n",
    "\n",
    "    Crisis periods are shaded on each panel.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(14, 12), sharex=True)\n",
    "    \n",
    "    # Panel 1: Mean correlation μ\n",
    "    ax = axes[0]\n",
    "    ax.plot(results_data['date'], results_data['mu'], color='orange', linewidth=0.8, label='μ')\n",
    "    ax.set_ylabel('μ (mean corr)', fontsize=11)\n",
    "    ax.set_title(f'Figure 5: Evolution of Market Indicators {title_suffix}', fontsize=14)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Panel 2: H - H_M\n",
    "    ax = axes[1]\n",
    "    ax.plot(results_data['date'], results_data['H_minus_HM'], color='blue', linewidth=0.8, label='H - H$_M$')\n",
    "    ax.set_ylabel('H - H$_M$', fontsize=11)\n",
    "    ax.axhline(y=0, color='black', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Panel 3: -ln(H - H_M) - \"fear gauge\" similar to VIX\n",
    "    ax = axes[2]\n",
    "    neg_ln = -np.log(results_data['H_minus_HM'].clip(lower=1e-10))\n",
    "    ax.plot(results_data['date'], neg_ln, color='red', linewidth=0.8, label='-ln(H - H$_M$) \"Fear Gauge\"')\n",
    "    ax.set_ylabel('-ln(H - H$_M$)', fontsize=11)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Panel 4: Largest eigenvalue\n",
    "    ax = axes[3]\n",
    "    ax.plot(results_data['date'], results_data['lambda_max'], color='green', linewidth=0.8, label='λ$_{max}$')\n",
    "    ax.set_ylabel('λ$_{max}$', fontsize=11)\n",
    "    ax.set_xlabel('Date', fontsize=11)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add crisis bands to all panels\n",
    "    crisis_added = False\n",
    "    for ax in axes:\n",
    "        for start, end, name in crisis_periods:\n",
    "            try:\n",
    "                if not crisis_added:\n",
    "                    ax.axvspan(pd.Timestamp(start), pd.Timestamp(end), \n",
    "                              color='lightcoral', alpha=0.3, label='Crisis Period')\n",
    "                    crisis_added = True\n",
    "                else:\n",
    "                    ax.axvspan(pd.Timestamp(start), pd.Timestamp(end), \n",
    "                              color='lightcoral', alpha=0.3)\n",
    "            except:\n",
    "                pass\n",
    "        crisis_added = False  # Reset for next panel\n",
    "    \n",
    "    # Add vertical lines for all crashes (red dashed lines)\n",
    "    for start, end, name in events_dict['crashes']:\n",
    "        try:\n",
    "            crash_date = pd.Timestamp(start)\n",
    "            for ax in axes:\n",
    "                ax.axvline(crash_date, color='red', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "            # Add label only to top panel for major crashes\n",
    "            if start in ['1987-10-19', '2000-03-10', '2008-09-15', '2020-02-20']:\n",
    "                short_name = name.split()[0] if len(name.split()) > 0 else name[:8]\n",
    "                axes[0].annotate(short_name, xy=(crash_date, axes[0].get_ylim()[1]), \n",
    "                                fontsize=7, ha='center', va='bottom', color='red', \n",
    "                                fontweight='bold', rotation=45)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Add vertical lines for all bubbles (blue dashed lines)\n",
    "    for start, end, name in events_dict['bubbles']:\n",
    "        try:\n",
    "            bubble_date = pd.Timestamp(start)\n",
    "            for ax in axes:\n",
    "                ax.axvline(bubble_date, color='blue', linestyle=':', linewidth=0.8, alpha=0.5)\n",
    "            # Add label only to top panel\n",
    "            short_name = name.split()[0] if len(name.split()) > 0 else name[:8]\n",
    "            axes[0].annotate(short_name, xy=(bubble_date, axes[0].get_ylim()[1] * 0.95), \n",
    "                            fontsize=7, ha='center', va='bottom', color='blue', \n",
    "                            fontweight='bold', rotation=45)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create pre-2005 plot\n",
    "create_plots(results_pre2005, events_pre2005, crisis_periods_pre2005, '(Pre-2005)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "See code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contains configuration parameters and utility functions for the pre-2005 analysis period.\n",
    "# It defines constants (such as date ranges and rolling window sizes), \n",
    "# a direction vector (u_hat) for transformation or projections,\n",
    "# and helper functions for:\n",
    "#   - computing rolling z-scores,\n",
    "#   - building date masks around crash events,\n",
    "#   - calculating Mahalanobis distances,\n",
    "#   - identifying if n out of the last k conditions are met (rolling),\n",
    "#   - and shading level regions on matplotlib axes.\n",
    "\n",
    "\n",
    "START_DATE = \"1985-01-02\"  # start date\n",
    "SHIFT = 10\n",
    "ZSCORE_WINDOW = 252\n",
    "PRECRASH_L_DAYS = 90\n",
    "TRAIN_END = \"2004-12-31\"\n",
    "PLOT_END  = \"2004-12-31\"\n",
    "\n",
    "COL_DATE = \"date\"\n",
    "COL_ABSH = \"abs_H_minus_HM\"\n",
    "COL_MU   = \"mu\"\n",
    "COL_L1   = \"lambda_max\"\n",
    "\n",
    "u_hat = np.array([-1.0, +1.0, +1.0])\n",
    "u_hat = u_hat / np.linalg.norm(u_hat)\n",
    "\n",
    "def rolling_zscore(s: pd.Series, window: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Compute the rolling z-score of a pandas Series.\n",
    "\n",
    "    Args:\n",
    "        s (pd.Series): Input series.\n",
    "        window (int): Window size for rolling calculation.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Series of rolling z-scores.\n",
    "    \"\"\"\n",
    "    minp = max(20, window // 5)\n",
    "    m = s.rolling(window, min_periods=minp).mean()\n",
    "    sd = s.rolling(window, min_periods=minp).std(ddof=0)\n",
    "    return (s - m) / sd\n",
    "\n",
    "def build_precrash_mask(index: pd.DatetimeIndex, crashes, L_days: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Builds a boolean mask identifying pre-crash periods for a given set of crashes.\n",
    "\n",
    "    Args:\n",
    "        index (pd.DatetimeIndex): Date index.\n",
    "        crashes (iterable): Iterable of (start, end, name) tuples for crash intervals.\n",
    "        L_days (int): Number of days before each crash to include in the mask.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Boolean series indexed by the given dates. True if in the pre-crash window.\n",
    "    \"\"\"\n",
    "    mask = pd.Series(False, index=index)\n",
    "    for start, end, name in crashes:\n",
    "        t0 = pd.to_datetime(start)\n",
    "        w0 = t0 - pd.Timedelta(days=L_days)\n",
    "        mask |= (index >= w0) & (index < t0)\n",
    "    return mask\n",
    "\n",
    "def mahalanobis_distance(X: np.ndarray, mu: np.ndarray, Sigma_inv: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the Mahalanobis distance of each row in X from the mean mu using provided inverse covariance.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Data array of shape (n_samples, n_features).\n",
    "        mu (np.ndarray): Mean vector of shape (n_features,).\n",
    "        Sigma_inv (np.ndarray): Inverse covariance matrix of shape (n_features, n_features).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 1D array of Mahalanobis distances.\n",
    "    \"\"\"\n",
    "    d = X - mu\n",
    "    q = np.einsum(\"ij,jk,ik->i\", d, Sigma_inv, d)\n",
    "    q = np.maximum(q, 0.0)\n",
    "    return np.sqrt(q)\n",
    "\n",
    "def n_out_of_k(condition: pd.Series, n: int, k: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Return a boolean Series indicating where at least n out of the last k conditions are True.\n",
    "\n",
    "    Args:\n",
    "        condition (pd.Series): Boolean condition series.\n",
    "        n (int): Minimum number of True observations required.\n",
    "        k (int): Size of the rolling window.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Boolean Series of same index as input.\n",
    "    \"\"\"\n",
    "    return condition.rolling(k, min_periods=k).sum() >= n\n",
    "\n",
    "def shade_levels(ax, idx, levels, alpha=0.10):\n",
    "    \"\"\"\n",
    "    Shade regions on a matplotlib axis according to level values.\n",
    "\n",
    "    Args:\n",
    "        ax: Matplotlib Axes object to which to add shaded regions.\n",
    "        idx (pd.Index): Index corresponding to the levels (typically datetime).\n",
    "        levels (pd.Series or np.array): Array-like object indicating level values (e.g., 1, 2, 3).\n",
    "        alpha (float): Transparency factor for shaded regions.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    colors = {1: \"gold\", 2: \"orange\", 3: \"red\"}\n",
    "    for L in [1, 2, 3]:\n",
    "        mask = (levels == L)\n",
    "        if not mask.any():\n",
    "            continue\n",
    "        seg = mask.astype(int).diff().fillna(mask.astype(int))\n",
    "        starts = idx[seg == 1]\n",
    "        ends   = idx[seg == -1]\n",
    "        if mask.iloc[0]:\n",
    "            starts = starts.insert(0, idx[0])\n",
    "        if mask.iloc[-1]:\n",
    "            ends = ends.insert(len(ends), idx[-1])\n",
    "        for s, e in zip(starts, ends):\n",
    "            ax.axvspan(s, e, color=colors[L], alpha=alpha, lw=0)\n",
    "\n",
    "\n",
    "# Prep + enforce start date\n",
    "r = results.copy()\n",
    "r[COL_DATE] = pd.to_datetime(r[COL_DATE])\n",
    "r = r.sort_values(COL_DATE).set_index(COL_DATE)\n",
    "\n",
    "# enforce start and (optional) end range early to keep everything consistent\n",
    "r = r.loc[pd.to_datetime(START_DATE):]\n",
    "\n",
    "base = r[[COL_ABSH, COL_MU, COL_L1]].dropna().copy()\n",
    "\n",
    "\n",
    "# State and velocity direction\n",
    "Xdf = pd.DataFrame(index=base.index)\n",
    "Xdf[\"z_absH\"] = rolling_zscore(base[COL_ABSH], ZSCORE_WINDOW)\n",
    "Xdf[\"z_mu\"]   = rolling_zscore(base[COL_MU],   ZSCORE_WINDOW)\n",
    "Xdf[\"z_l1\"]   = rolling_zscore(base[COL_L1],   ZSCORE_WINDOW)\n",
    "Xdf = Xdf.dropna()\n",
    "\n",
    "Vdf = (Xdf - Xdf.shift(SHIFT)) / SHIFT\n",
    "Vdf = Vdf.dropna()\n",
    "\n",
    "dt_to = (Vdf[[\"z_absH\", \"z_mu\", \"z_l1\"]].values @ u_hat)\n",
    "dt_to = pd.Series(dt_to, index=Vdf.index, name=\"d_to\")\n",
    "\n",
    "# align X to velocity index\n",
    "Xdf2 = Xdf.loc[Vdf.index].copy()\n",
    "\n",
    "\n",
    "# Fit crash region on TRAIN only\n",
    "train_idx = Xdf2.index <= pd.to_datetime(TRAIN_END)\n",
    "X_train = Xdf2.loc[train_idx]\n",
    "\n",
    "precrash_mask = build_precrash_mask(X_train.index, MARKET_EVENTS[\"crashes\"], PRECRASH_L_DAYS)\n",
    "X_precrash = X_train.loc[precrash_mask]\n",
    "\n",
    "if len(X_precrash) < 30:\n",
    "    raise ValueError(\n",
    "        f\"Too few pre-crash training points ({len(X_precrash)}). \"\n",
    "        f\"Increase TRAIN span or PRECRASH_L_DAYS, or reduce zscore window.\"\n",
    "    )\n",
    "\n",
    "mu_c = X_precrash.mean(axis=0).values\n",
    "Sigma_c = np.cov(X_precrash.values.T)\n",
    "\n",
    "# regularize covariance\n",
    "eps = 1e-6\n",
    "Sigma_inv = np.linalg.inv(Sigma_c + eps * np.eye(Sigma_c.shape[0]))\n",
    "\n",
    "\n",
    "# Score + thresholds\n",
    "plot_slice = (Xdf2.index <= pd.to_datetime(PLOT_END))\n",
    "X_plot = Xdf2.loc[plot_slice]\n",
    "\n",
    "D = pd.Series(mahalanobis_distance(X_plot.values, mu_c, Sigma_inv),\n",
    "              index=X_plot.index, name=\"D_mahal\")\n",
    "\n",
    "R_exp = np.exp(-D)\n",
    "R_exp.name = \"R_exp\"\n",
    "\n",
    "R_train = R_exp.loc[R_exp.index <= pd.to_datetime(TRAIN_END)]\n",
    "q90, q95, q98 = R_train.quantile([0.90, 0.95, 0.98]).values\n",
    "\n",
    "thr_watch = (R_exp > q90)\n",
    "thr_elev  = (R_exp > q95)\n",
    "thr_alert = (R_exp > q98)\n",
    "\n",
    "# persistence rules\n",
    "watch_persist = n_out_of_k(thr_watch, n=2, k=3)\n",
    "elev_persist  = n_out_of_k(thr_elev,  n=3, k=4)\n",
    "\n",
    "dt_to_plot = dt_to.reindex(R_exp.index)\n",
    "alert_persist = thr_alert & thr_alert.shift(1).fillna(False) & (dt_to_plot > 0)\n",
    "\n",
    "level_persist = pd.Series(0, index=R_exp.index)\n",
    "level_persist.loc[watch_persist] = 1\n",
    "level_persist.loc[elev_persist]  = 2\n",
    "level_persist.loc[alert_persist] = 3\n",
    "\n",
    "\n",
    "# Plot (pre-2005)\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 11), sharex=True)\n",
    "\n",
    "axes[0].plot(R_exp.index, R_exp.values, lw=1.6, color=\"tab:blue\", label=\"R_exp = exp(-D)\")\n",
    "axes[0].axhline(q90, color=\"gold\",   ls=\"--\", lw=1.0, label=\"q90 (train)\")\n",
    "axes[0].axhline(q95, color=\"orange\", ls=\"--\", lw=1.0, label=\"q95 (train)\")\n",
    "axes[0].axhline(q98, color=\"red\",    ls=\"--\", lw=1.0, label=\"q98 (train)\")\n",
    "shade_levels(axes[0], level_persist.index, level_persist, alpha=0.10)\n",
    "axes[0].set_ylabel(\"Risk (higher=closer)\")\n",
    "axes[0].set_title(f\"Mahalanobis Risk + Persistence Warnings (from {START_DATE} to {PLOT_END})\")\n",
    "axes[0].legend(loc=\"upper left\", ncol=2)\n",
    "\n",
    "axes[1].plot(D.index, D.values, lw=1.4, color=\"tab:purple\", label=\"D (Mahalanobis distance)\")\n",
    "axes[1].set_ylabel(\"Distance (lower=closer)\")\n",
    "axes[1].legend(loc=\"upper left\")\n",
    "\n",
    "dt_to_pre = dt_to_plot.loc[:PLOT_END]\n",
    "axes[2].plot(dt_to_pre.index, dt_to_pre.values, lw=1.2, color=\"tab:green\", label=r\"$d^{\\rightarrow}_t = v_t \\cdot \\hat{u}$\")\n",
    "axes[2].axhline(0, color=\"k\", lw=0.8, alpha=0.4)\n",
    "axes[2].set_ylabel(\"Toward-crash velocity\")\n",
    "axes[2].legend(loc=\"upper left\")\n",
    "\n",
    "# optional: shade crash windows too (only those that overlap plot range)\n",
    "for start, end, name in MARKET_EVENTS[\"crashes\"]:\n",
    "    s = pd.to_datetime(start); e = pd.to_datetime(end)\n",
    "    if e >= pd.to_datetime(START_DATE) and s <= pd.to_datetime(PLOT_END):\n",
    "        for ax in axes:\n",
    "            ax.axvspan(max(s, pd.to_datetime(START_DATE)),\n",
    "                       min(e, pd.to_datetime(PLOT_END)),\n",
    "                       color=\"black\", alpha=0.05, lw=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Inspect signals table (tail)\n",
    "signals = pd.DataFrame({\n",
    "    \"R_exp\": R_exp,\n",
    "    \"D\": D,\n",
    "    \"d_to\": dt_to_plot,\n",
    "    \"level_persist\": level_persist\n",
    "})\n",
    "display(signals.tail(20))\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(R_exp.index, R_exp, label=\"R_exp\")\n",
    "plt.axhline(q98, color=\"red\", ls=\"--\", label=\"q98\")\n",
    "plt.scatter(R_exp.index[thr_alert], R_exp[thr_alert], s=12, color=\"red\", alpha=0.6, label=\">q98 (raw)\")\n",
    "plt.scatter(R_exp.index[alert_persist], R_exp[alert_persist], s=35, color=\"black\", label=\"ALERT (persist)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "Alert–crash matching and recall/precision; risk plot with crash windows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46defd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Settings for validation\n",
    "LEAD_MAX_DAYS = 90          # how far ahead an alert can count as predicting the crash\n",
    "MIN_GAP_DAYS = 5            # de-duplicate alert clusters (cooldown)\n",
    "USE_LEVEL = \"level_persist\" # column name in `signals` table OR use `level_persist` Series directly\n",
    "\n",
    "\n",
    "# Build crash event table\n",
    "crashes = []\n",
    "for start, end, name in MARKET_EVENTS[\"crashes\"]:\n",
    "    crashes.append({\n",
    "        \"name\": name,\n",
    "        \"start\": pd.to_datetime(start),\n",
    "        \"end\": pd.to_datetime(end),\n",
    "    })\n",
    "crash_df = pd.DataFrame(crashes).sort_values(\"start\").reset_index(drop=True)\n",
    "\n",
    "# Restrict to your data range (optional)\n",
    "data_start = signals.index.min()\n",
    "data_end   = signals.index.max()\n",
    "crash_df = crash_df[(crash_df[\"end\"] >= data_start) & (crash_df[\"start\"] <= data_end)].copy()\n",
    "\n",
    "# Extract ALERT timestamps (level==3) and de-duplicate\n",
    "if isinstance(signals, pd.DataFrame) and USE_LEVEL in signals.columns:\n",
    "    lvl = signals[USE_LEVEL].copy()\n",
    "else:\n",
    "    # if you have a Series named level_persist\n",
    "    lvl = level_persist.copy()\n",
    "\n",
    "alerts = lvl[lvl == 3].index.to_series().sort_values()\n",
    "\n",
    "# Cooldown to avoid counting 10 consecutive-alert days as 10 predictions\n",
    "if len(alerts) > 0 and MIN_GAP_DAYS is not None and MIN_GAP_DAYS > 0:\n",
    "    filtered = [alerts.iloc[0]]\n",
    "    for t in alerts.iloc[1:]:\n",
    "        if (t - filtered[-1]).days >= MIN_GAP_DAYS:\n",
    "            filtered.append(t)\n",
    "    alerts = pd.to_datetime(filtered)\n",
    "\n",
    "alerts = pd.DatetimeIndex(alerts)\n",
    "\n",
    "print(f\"#alerts (deduped) = {len(alerts)}\")\n",
    "print(f\"#crashes in-range = {len(crash_df)}\")\n",
    "\n",
    "\n",
    "# Matching logic\n",
    "# For each crash: find alerts in (start-LEAD_MAX_DAYS, start)\n",
    "rows = []\n",
    "for _, ev in crash_df.iterrows():\n",
    "    start = ev[\"start\"]\n",
    "    w0 = start - pd.Timedelta(days=LEAD_MAX_DAYS)\n",
    "    # alerts strictly before crash start\n",
    "    cand = alerts[(alerts >= w0) & (alerts < start)]\n",
    "    if len(cand) == 0:\n",
    "        rows.append({\n",
    "            \"crash\": ev[\"name\"],\n",
    "            \"crash_start\": start,\n",
    "            \"hit\": False,\n",
    "            \"earliest_alert\": pd.NaT,\n",
    "            \"lead_time_days\": np.nan,\n",
    "            \"n_alerts_in_window\": 0\n",
    "        })\n",
    "    else:\n",
    "        earliest = cand.min()\n",
    "        rows.append({\n",
    "            \"crash\": ev[\"name\"],\n",
    "            \"crash_start\": start,\n",
    "            \"hit\": True,\n",
    "            \"earliest_alert\": earliest,\n",
    "            \"lead_time_days\": (start - earliest).days,\n",
    "            \"n_alerts_in_window\": len(cand)\n",
    "        })\n",
    "\n",
    "crash_match = pd.DataFrame(rows)\n",
    "\n",
    "# Recall: fraction of crashes hit\n",
    "recall = crash_match[\"hit\"].mean() if len(crash_match) else np.nan\n",
    "\n",
    "# Precision: fraction of alerts that match at least one crash within LEAD_MAX_DAYS after alert\n",
    "# For each alert t, see if exists crash start in (t, t+LEAD_MAX_DAYS]\n",
    "alert_hits = []\n",
    "for t in alerts:\n",
    "    future_crashes = crash_df[(crash_df[\"start\"] > t) & (crash_df[\"start\"] <= t + pd.Timedelta(days=LEAD_MAX_DAYS))]\n",
    "    alert_hits.append(len(future_crashes) > 0)\n",
    "\n",
    "alert_hits = np.array(alert_hits, dtype=bool)\n",
    "precision = alert_hits.mean() if len(alert_hits) else np.nan\n",
    "\n",
    "# False alarms per year (optional, nice in imbalanced setting)\n",
    "years = max(1e-9, (data_end - data_start).days / 365.25)\n",
    "false_alarms_per_year = ((~alert_hits).sum() / years) if len(alert_hits) else np.nan\n",
    "\n",
    "print(\"\\n=== Validation summary ===\")\n",
    "print(f\"Lead window (days): {LEAD_MAX_DAYS}\")\n",
    "print(f\"Recall (crashes predicted): {recall:.3f}\")\n",
    "print(f\"Precision (alerts that precede a crash): {precision:.3f}\")\n",
    "print(f\"False alarms / year: {false_alarms_per_year:.2f}\")\n",
    "\n",
    "display(crash_match)\n",
    "\n",
    "\n",
    "# Plot: risk + crash windows + ALERT markers\n",
    "# Choose a plot range \n",
    "plot_start = pd.to_datetime(\"1985-01-02\")\n",
    "plot_end   = pd.to_datetime(\"2004-12-31\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 5))\n",
    "\n",
    "# plot risk if available\n",
    "if \"R_exp\" in signals.columns:\n",
    "    s = signals.loc[plot_start:plot_end, \"R_exp\"]\n",
    "    ax.plot(s.index, s.values, lw=1.3, label=\"R_exp\")\n",
    "\n",
    "# shade crash windows\n",
    "for _, ev in crash_df.iterrows():\n",
    "    s = ev[\"start\"]; e = ev[\"end\"]\n",
    "    if e < plot_start or s > plot_end:\n",
    "        continue\n",
    "    ax.axvspan(max(s, plot_start), min(e, plot_end), color=\"black\", alpha=0.07)\n",
    "\n",
    "# mark alerts\n",
    "alert_plot = alerts[(alerts >= plot_start) & (alerts <= plot_end)]\n",
    "if len(alert_plot) > 0:\n",
    "    y = np.nan\n",
    "    if \"R_exp\" in signals.columns and len(s) > 0:\n",
    "        # place markers at the curve level\n",
    "        yvals = signals.loc[alert_plot, \"R_exp\"].values\n",
    "        ax.scatter(alert_plot, yvals, color=\"red\", s=25, label=\"ALERT (deduped)\", zorder=5)\n",
    "    else:\n",
    "        ax.scatter(alert_plot, np.ones(len(alert_plot)), color=\"red\", s=25, label=\"ALERT (deduped)\", zorder=5)\n",
    "\n",
    "ax.set_title(\"Risk score with crash windows (shaded) and ALERT markers\")\n",
    "ax.set_ylabel(\"R_exp (higher = closer)\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D around Dot-com crash\n",
    "\n",
    "Mahalanobis distance around March 2000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e41881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mahalonobis distance around Dot-com crash\n",
    "print(\"Min D (pre-2005):\", D.min())\n",
    "print(\"Min D during pre-crash windows:\")\n",
    "\n",
    "mask_pre = build_precrash_mask(D.index, MARKET_EVENTS[\"crashes\"], PRECRASH_L_DAYS)\n",
    "print(D[mask_pre].describe())\n",
    "\n",
    "# Plot D around 1987 crash\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(D.loc[\"1999\":\"2000\"])\n",
    "plt.axvline(pd.Timestamp(\"2000-03-10\"), color=\"r\", ls=\"--\")\n",
    "plt.title(\"Mahalanobis distance around Dot-com crash\")\n",
    "plt.ylabel(\"D (lower = closer)\")\n",
    "plt.ylim(0, 11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D during normal period\n",
    "\n",
    "Distance during a calm window (1994–1995) for baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9183ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose a NORMAL reference period\n",
    "# Strategy:\n",
    "# 1) Exclude all pre-crash windows and crash windows\n",
    "# 2) Pick a contiguous window of similar length as the crash plot\n",
    "# 3) Plot D over that window\n",
    "\n",
    "NORMAL_START = pd.Timestamp(\"1994-01-01\")\n",
    "NORMAL_END   = pd.Timestamp(\"1995-12-31\")  # ~2 years, quiet market\n",
    "\n",
    "# Build exclusion masks\n",
    "mask_pre = build_precrash_mask(D.index, MARKET_EVENTS[\"crashes\"], PRECRASH_L_DAYS)\n",
    "\n",
    "def build_crash_mask(index, crashes):\n",
    "    m = pd.Series(False, index=index)\n",
    "    for start, end, _ in crashes:\n",
    "        s = pd.to_datetime(start)\n",
    "        e = pd.to_datetime(end)\n",
    "        m |= (index >= s) & (index <= e)\n",
    "    return m\n",
    "\n",
    "mask_crash = build_crash_mask(D.index, MARKET_EVENTS[\"crashes\"])\n",
    "\n",
    "mask_normal = (~mask_pre) & (~mask_crash)\n",
    "\n",
    "# Extract normal-period data\n",
    "D_normal_period = D.loc[NORMAL_START:NORMAL_END]\n",
    "D_normal_period = D_normal_period[mask_normal.loc[D_normal_period.index]]\n",
    "\n",
    "print(f\"Normal-period points: {len(D_normal_period)}\")\n",
    "print(D_normal_period.describe())\n",
    "\n",
    "\n",
    "# Plot: normal period\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(D_normal_period.index, D_normal_period.values, lw=1.5, color=\"steelblue\")\n",
    "plt.title(\"Mahalanobis distance during a normal market period (1994–1995)\")\n",
    "plt.ylabel(\"D (lower = closer to pre-crash centroid)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylim(0, 11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of D\n",
    "\n",
    "Histograms of D in pre-crash vs normal periods; separation metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809dd159",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pre-crash mask \n",
    "mask_pre = build_precrash_mask(D.index, MARKET_EVENTS[\"crashes\"], PRECRASH_L_DAYS)\n",
    "\n",
    "D_pre = D[mask_pre].dropna()\n",
    "D_norm = D[~mask_pre].dropna()\n",
    "\n",
    "print(\"Counts:\")\n",
    "print(\"  pre-crash points:\", len(D_pre))\n",
    "print(\"  normal points   :\", len(D_norm))\n",
    "print(\"\\nSummary:\")\n",
    "print(\"  D pre-crash describe:\\n\", D_pre.describe())\n",
    "print(\"\\n  D normal describe:\\n\", D_norm.describe())\n",
    "\n",
    "# 2) Overlay histograms\n",
    "plt.figure(figsize=(10,4))\n",
    "bins = 40\n",
    "plt.hist(D_norm, bins=bins, alpha=0.5, label=\"Normal\", density=True)\n",
    "plt.hist(D_pre,  bins=bins, alpha=0.5, label=\"Pre-crash\", density=True)\n",
    "plt.title(\"Distribution of Mahalanobis distance D\\n(lower = closer to crash centroid)\")\n",
    "plt.xlabel(\"D\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Separation metric \n",
    "# Probability that a random pre-crash point has smaller D than a random normal point.\n",
    "def auc_like(a, b):\n",
    "    # a=pre, b=normal; returns P(a<b)\n",
    "    a = np.asarray(a); b = np.asarray(b)\n",
    "    return np.mean(a[:,None] < b[None,:])\n",
    "\n",
    "if len(D_pre) > 0 and len(D_norm) > 0 and len(D_pre)*len(D_norm) <= 2_000_000:\n",
    "    print(\"\\nSeparation P(D_pre < D_norm):\", auc_like(D_pre.values, D_norm.values))\n",
    "else:\n",
    "    print(\"\\nSkipping separation calc (too many points).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Phase space plot for Dot Com Crash\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a8d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters \n",
    "EVENT_NAME = \"Dot-com crash\"\n",
    "CRASH_DAY  = pd.Timestamp(\"2000-03-10\")\n",
    "WINDOW_DAYS = 120          # window shown around crash for \"trajectory + last-step arrow\"\n",
    "PRECRASH_L_DAYS_LOCAL = 60 # pre-crash label window used for the cloud (can match your earlier choice)\n",
    "USE_TRAIN_ONLY = True\n",
    "TRAIN_END_LOCAL = pd.Timestamp(\"2004-12-31\")\n",
    "\n",
    "# Helper function (pca_2d defined in Cell 8)\n",
    "def pca_2d(X: np.ndarray):\n",
    "    \"\"\"Return 2D PCA projection of X using SVD (no sklearn dependency).\"\"\"\n",
    "    Xc = X - X.mean(axis=0, keepdims=True)\n",
    "    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
    "    W = Vt[:2].T               # 3x2\n",
    "    Y = Xc @ W                 # Nx2\n",
    "    return Y, W, X.mean(axis=0)\n",
    "\n",
    "\n",
    "# Select data to plot (train-only optional)\n",
    "X_use = Xdf2[[\"z_absH\", \"z_mu\", \"z_l1\"]].dropna().copy()\n",
    "if USE_TRAIN_ONLY:\n",
    "    X_use = X_use.loc[:TRAIN_END_LOCAL]\n",
    "\n",
    "# Create pre-crash mask from your crash list\n",
    "mask_pre = build_precrash_mask(X_use.index, MARKET_EVENTS[\"crashes\"], PRECRASH_L_DAYS_LOCAL)\n",
    "mask_norm = ~mask_pre\n",
    "\n",
    "# Ensure crash day exists (or use nearest)\n",
    "if CRASH_DAY not in X_use.index:\n",
    "    # use nearest available timestamp (common when data is in epochs)\n",
    "    nearest = X_use.index[np.argmin(np.abs((X_use.index - CRASH_DAY).days))]\n",
    "    print(f\"CRASH_DAY {CRASH_DAY.date()} not found; using nearest {nearest.date()}\")\n",
    "    CRASH_DAY = nearest\n",
    "\n",
    "\n",
    "# PCA projection for schematic readability\n",
    "Y, W, mean_x = pca_2d(X_use.values)\n",
    "Ydf = pd.DataFrame(Y, index=X_use.index, columns=[\"PC1\", \"PC2\"])\n",
    "\n",
    "# Project pre-crash centroid and crash point\n",
    "pre_centroid = X_use.loc[mask_pre].mean(axis=0).values\n",
    "crash_point = X_use.loc[CRASH_DAY].values\n",
    "\n",
    "def project_point(x):\n",
    "    # x: (3,) in original space; project into PC space\n",
    "    xc = x - mean_x\n",
    "    return xc @ W\n",
    "\n",
    "pre_centroid_2d = project_point(pre_centroid)\n",
    "crash_point_2d = project_point(crash_point)\n",
    "\n",
    "\n",
    "# Last-step velocity arrow into crash (in PC space)\n",
    "# Take previous point in time as \"approach\"\n",
    "prev_idx = X_use.index.get_loc(CRASH_DAY) - 1\n",
    "if prev_idx < 0:\n",
    "    prev_day = CRASH_DAY\n",
    "else:\n",
    "    prev_day = X_use.index[prev_idx]\n",
    "prev_point = X_use.loc[prev_day].values\n",
    "prev_point_2d = project_point(prev_point)\n",
    "\n",
    "\n",
    "# Optional local trajectory window around crash\n",
    "t0 = CRASH_DAY - pd.Timedelta(days=WINDOW_DAYS)\n",
    "t1 = CRASH_DAY + pd.Timedelta(days=WINDOW_DAYS)\n",
    "traj = Ydf.loc[t0:t1]\n",
    "\n",
    "# Plot schematic\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "# Clouds\n",
    "ax.scatter(Ydf.loc[mask_norm, \"PC1\"], Ydf.loc[mask_norm, \"PC2\"],\n",
    "           s=10, alpha=0.18, color=\"gray\", label=\"Normal (non pre-crash)\")\n",
    "ax.scatter(Ydf.loc[mask_pre, \"PC1\"], Ydf.loc[mask_pre, \"PC2\"],\n",
    "           s=14, alpha=0.35, color=\"orange\", label=f\"Pre-crash (last {PRECRASH_L_DAYS_LOCAL}d before crash starts)\")\n",
    "\n",
    "# Local trajectory around crash (thin line)\n",
    "if len(traj) > 2:\n",
    "    ax.plot(traj[\"PC1\"], traj[\"PC2\"], lw=1.2, alpha=0.65, color=\"steelblue\", label=f\"Local trajectory (±{WINDOW_DAYS}d)\")\n",
    "\n",
    "# Centroid and crash point\n",
    "ax.scatter(pre_centroid_2d[0], pre_centroid_2d[1], s=140, color=\"orange\", edgecolor=\"k\", zorder=6, marker=\"o\", label=\"Pre-crash centroid\")\n",
    "ax.scatter(crash_point_2d[0], crash_point_2d[1], s=220, color=\"red\", edgecolor=\"k\", zorder=7, marker=\"*\", label=f\"Crash day ({EVENT_NAME})\")\n",
    "\n",
    "# Arrow: centroid -> crash point (the “jump” schematic)\n",
    "ax.annotate(\"\",\n",
    "            xy=(crash_point_2d[0], crash_point_2d[1]),\n",
    "            xytext=(pre_centroid_2d[0], pre_centroid_2d[1]),\n",
    "            arrowprops=dict(arrowstyle=\"->\", lw=2.6, color=\"red\", alpha=0.85))\n",
    "ax.text((pre_centroid_2d[0] + crash_point_2d[0]) / 2,\n",
    "        (pre_centroid_2d[1] + crash_point_2d[1]) / 2,\n",
    "        \"jump to crash state\",\n",
    "        color=\"red\", fontsize=10, ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# Arrow: previous -> crash point (last-step approach)\n",
    "ax.annotate(\"\",\n",
    "            xy=(crash_point_2d[0], crash_point_2d[1]),\n",
    "            xytext=(prev_point_2d[0], prev_point_2d[1]),\n",
    "            arrowprops=dict(arrowstyle=\"->\", lw=2.0, color=\"black\", alpha=0.75))\n",
    "ax.text(prev_point_2d[0], prev_point_2d[1], \"last step\", fontsize=9, color=\"black\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.set_title(\"Schematic: pre-crash cloud vs crash point in phase space (PCA projection)\")\n",
    "ax.set_xlabel(\"PC1 (from z(|H-HM|), z(mu), z(lambda_max))\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.legend(loc=\"best\", frameon=True)\n",
    "ax.grid(True, alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Phase space plot for Black Monday\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ddf374",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters \n",
    "EVENT_NAME = \"Black Monday\"\n",
    "CRASH_DAY  = pd.Timestamp(\"1987-10-19\")\n",
    "WINDOW_DAYS = 120          # window shown around crash for \"trajectory + last-step arrow\"\n",
    "PRECRASH_L_DAYS_LOCAL = 60 # pre-crash label window used for the cloud (can match your earlier choice)\n",
    "USE_TRAIN_ONLY = True\n",
    "TRAIN_END_LOCAL = pd.Timestamp(\"2004-12-31\")\n",
    "\n",
    "\n",
    "# 1) Select data to plot (train-only optional)\n",
    "X_use = Xdf2[[\"z_absH\", \"z_mu\", \"z_l1\"]].dropna().copy()\n",
    "if USE_TRAIN_ONLY:\n",
    "    X_use = X_use.loc[:TRAIN_END_LOCAL]\n",
    "\n",
    "# Create pre-crash mask from your crash list\n",
    "mask_pre = build_precrash_mask(X_use.index, MARKET_EVENTS[\"crashes\"], PRECRASH_L_DAYS_LOCAL)\n",
    "mask_norm = ~mask_pre\n",
    "\n",
    "# Ensure crash day exists (or use nearest)\n",
    "if CRASH_DAY not in X_use.index:\n",
    "    # use nearest available timestamp (common when data is in epochs)\n",
    "    nearest = X_use.index[np.argmin(np.abs((X_use.index - CRASH_DAY).days))]\n",
    "    print(f\"CRASH_DAY {CRASH_DAY.date()} not found; using nearest {nearest.date()}\")\n",
    "    CRASH_DAY = nearest\n",
    "\n",
    "\n",
    "# 2) PCA projection for schematic readability\n",
    "Y, W, mean_x = pca_2d(X_use.values)\n",
    "Ydf = pd.DataFrame(Y, index=X_use.index, columns=[\"PC1\", \"PC2\"])\n",
    "\n",
    "# Project pre-crash centroid and crash point\n",
    "pre_centroid = X_use.loc[mask_pre].mean(axis=0).values\n",
    "crash_point = X_use.loc[CRASH_DAY].values\n",
    "\n",
    "def project_point(x):\n",
    "    # x: (3,) in original space; project into PC space\n",
    "    xc = x - mean_x\n",
    "    return xc @ W\n",
    "\n",
    "pre_centroid_2d = project_point(pre_centroid)\n",
    "crash_point_2d = project_point(crash_point)\n",
    "\n",
    "\n",
    "# Last-step velocity arrow into crash (in PC space)\n",
    "# Take previous point in time as \"approach\"\n",
    "prev_idx = X_use.index.get_loc(CRASH_DAY) - 1\n",
    "if prev_idx < 0:\n",
    "    prev_day = CRASH_DAY\n",
    "else:\n",
    "    prev_day = X_use.index[prev_idx]\n",
    "prev_point = X_use.loc[prev_day].values\n",
    "prev_point_2d = project_point(prev_point)\n",
    "\n",
    "\n",
    "# Local trajectory window around crash\n",
    "\n",
    "t0 = CRASH_DAY - pd.Timedelta(days=WINDOW_DAYS)\n",
    "t1 = CRASH_DAY + pd.Timedelta(days=WINDOW_DAYS)\n",
    "traj = Ydf.loc[t0:t1]\n",
    "\n",
    "# Plot Shematic\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "# Clouds\n",
    "ax.scatter(Ydf.loc[mask_norm, \"PC1\"], Ydf.loc[mask_norm, \"PC2\"],\n",
    "           s=10, alpha=0.18, color=\"gray\", label=\"Normal (non pre-crash)\")\n",
    "ax.scatter(Ydf.loc[mask_pre, \"PC1\"], Ydf.loc[mask_pre, \"PC2\"],\n",
    "           s=14, alpha=0.35, color=\"orange\", label=f\"Pre-crash (last {PRECRASH_L_DAYS_LOCAL}d before crash starts)\")\n",
    "\n",
    "# Local trajectory around crash (thin line)\n",
    "if len(traj) > 2:\n",
    "    ax.plot(traj[\"PC1\"], traj[\"PC2\"], lw=1.2, alpha=0.65, color=\"steelblue\", label=f\"Local trajectory (±{WINDOW_DAYS}d)\")\n",
    "\n",
    "# Centroid and crash point\n",
    "ax.scatter(pre_centroid_2d[0], pre_centroid_2d[1], s=140, color=\"orange\", edgecolor=\"k\", zorder=6, marker=\"o\", label=\"Pre-crash centroid\")\n",
    "ax.scatter(crash_point_2d[0], crash_point_2d[1], s=220, color=\"red\", edgecolor=\"k\", zorder=7, marker=\"*\", label=f\"Crash day ({EVENT_NAME})\")\n",
    "\n",
    "# Arrow: centroid -> crash point (the “jump” schematic)\n",
    "ax.annotate(\"\",\n",
    "            xy=(crash_point_2d[0], crash_point_2d[1]),\n",
    "            xytext=(pre_centroid_2d[0], pre_centroid_2d[1]),\n",
    "            arrowprops=dict(arrowstyle=\"->\", lw=2.6, color=\"red\", alpha=0.85))\n",
    "ax.text((pre_centroid_2d[0] + crash_point_2d[0]) / 2,\n",
    "        (pre_centroid_2d[1] + crash_point_2d[1]) / 2,\n",
    "        \"jump to crash state\",\n",
    "        color=\"red\", fontsize=10, ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# Arrow: previous -> crash point (last-step approach)\n",
    "ax.annotate(\"\",\n",
    "            xy=(crash_point_2d[0], crash_point_2d[1]),\n",
    "            xytext=(prev_point_2d[0], prev_point_2d[1]),\n",
    "            arrowprops=dict(arrowstyle=\"->\", lw=2.0, color=\"black\", alpha=0.75))\n",
    "ax.text(prev_point_2d[0], prev_point_2d[1], \"last step\", fontsize=9, color=\"black\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.set_title(\"Schematic: pre-crash cloud vs crash point in phase space (PCA projection)\")\n",
    "ax.set_xlabel(\"PC1 (from z(|H-HM|), z(mu), z(lambda_max))\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.legend(loc=\"best\", frameon=True)\n",
    "ax.grid(True, alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event-aligned D\n",
    "\n",
    "Mahalanobis distance aligned at crash start; median and IQR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0122f2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Event-aligned Mahalanobis distance across crashes + \"normal-period\" reference band ---\n",
    "# Adds a reference distribution from NORMAL periods (times not in any pre-crash window and not in crash windows),\n",
    "# shown as:\n",
    "#   (A) horizontal dashed lines at the normal median and IQR\n",
    "#   (B) optional normalization of each crash curve by its own pre-window median (already supported)\n",
    "\n",
    "\n",
    "\n",
    "# Settings\n",
    "ALIGN_WINDOW_DAYS = 180\n",
    "RESAMPLE = \"D\"\n",
    "MIN_POINTS = 30\n",
    "NORMALIZE = False\n",
    "\n",
    "# Normal reference construction\n",
    "PRECRASH_L_DAYS_REF = 60     # pre-crash mask width \n",
    "EXCLUDE_CRASH_DAYS = True    # also exclude crash windows themselves from \"normal\"\n",
    "USE_TRAIN_ONLY_FOR_NORMAL = True\n",
    "TRAIN_END_LOCAL = pd.Timestamp(\"2004-12-31\")\n",
    "\n",
    "\n",
    "# Helpers\n",
    "def build_crashwindow_mask(index: pd.DatetimeIndex, crashes) -> pd.Series:\n",
    "    mask = pd.Series(False, index=index)\n",
    "    for start, end, name in crashes:\n",
    "        s = pd.to_datetime(start)\n",
    "        e = pd.to_datetime(end)\n",
    "        mask |= (index >= s) & (index <= e)\n",
    "    return mask\n",
    "\n",
    "\n",
    "# Data + event list\n",
    "crash_events = [(pd.to_datetime(s), pd.to_datetime(e), name) for s, e, name in MARKET_EVENTS[\"crashes\"]]\n",
    "crash_events = sorted(crash_events, key=lambda x: x[0])\n",
    "\n",
    "D_use = D.dropna().sort_index()\n",
    "if USE_TRAIN_ONLY_FOR_NORMAL:\n",
    "    D_use_for_normal = D_use.loc[:TRAIN_END_LOCAL].copy()\n",
    "else:\n",
    "    D_use_for_normal = D_use.copy()\n",
    "\n",
    "# Normal reference distribution (global, not aligned)\n",
    "mask_pre_ref = build_precrash_mask(D_use_for_normal.index, MARKET_EVENTS[\"crashes\"], PRECRASH_L_DAYS_REF)\n",
    "mask_norm_ref = ~mask_pre_ref\n",
    "\n",
    "if EXCLUDE_CRASH_DAYS:\n",
    "    mask_crash = build_crashwindow_mask(D_use_for_normal.index, MARKET_EVENTS[\"crashes\"])\n",
    "    mask_norm_ref &= ~mask_crash\n",
    "\n",
    "D_normal_ref = D_use_for_normal.loc[mask_norm_ref].dropna()\n",
    "\n",
    "if len(D_normal_ref) < 50:\n",
    "    print(\"Warning: very few normal reference points; consider setting USE_TRAIN_ONLY_FOR_NORMAL=False \"\n",
    "          \"or EXCLUDE_CRASH_DAYS=False, or increasing your data range.\")\n",
    "\n",
    "norm_med = D_normal_ref.median()\n",
    "norm_q25 = D_normal_ref.quantile(0.25)\n",
    "norm_q75 = D_normal_ref.quantile(0.75)\n",
    "\n",
    "print(f\"Normal reference (global) median={norm_med:.3f}, IQR=[{norm_q25:.3f}, {norm_q75:.3f}] \"\n",
    "      f\"(N={len(D_normal_ref)})\")\n",
    "\n",
    "# Build aligned matrix: rows = relative day, cols = crash\n",
    "t_grid = pd.timedelta_range(start=-pd.Timedelta(days=ALIGN_WINDOW_DAYS),\n",
    "                            end=pd.Timedelta(days=ALIGN_WINDOW_DAYS),\n",
    "                            freq=RESAMPLE)\n",
    "rel_days = np.array([td.days for td in t_grid], dtype=int)\n",
    "\n",
    "aligned = {}\n",
    "meta_rows = []\n",
    "\n",
    "for start, end, name in crash_events:\n",
    "    w0 = start - pd.Timedelta(days=ALIGN_WINDOW_DAYS)\n",
    "    w1 = start + pd.Timedelta(days=ALIGN_WINDOW_DAYS)\n",
    "\n",
    "    seg = D_use.loc[w0:w1].copy()\n",
    "\n",
    "    if RESAMPLE is not None:\n",
    "        seg = seg.reindex(pd.date_range(w0, w1, freq=RESAMPLE),\n",
    "                          method=\"nearest\",\n",
    "                          tolerance=pd.Timedelta(days=3))\n",
    "\n",
    "    seg_rel = (seg.index - start).days\n",
    "    seg.index = seg_rel\n",
    "\n",
    "    y = pd.Series(index=rel_days, dtype=float)\n",
    "    y.loc[seg.index.intersection(rel_days)] = seg.values\n",
    "\n",
    "    npts = y.notna().sum()\n",
    "    if npts < MIN_POINTS:\n",
    "        continue\n",
    "\n",
    "    if NORMALIZE:\n",
    "        baseline = y.loc[-ALIGN_WINDOW_DAYS:-1].median()\n",
    "        if np.isfinite(baseline) and baseline > 0:\n",
    "            y = y / baseline\n",
    "\n",
    "    aligned[name] = y\n",
    "\n",
    "    pre_med = y.loc[-60:-1].median()\n",
    "    post_med = y.loc[0:60].median()\n",
    "    spike = y.loc[0:7].max()\n",
    "    meta_rows.append({\n",
    "        \"crash\": name,\n",
    "        \"start\": start.date(),\n",
    "        \"points_in_window\": int(npts),\n",
    "        \"median_D_pre60\": float(pre_med) if np.isfinite(pre_med) else np.nan,\n",
    "        \"median_D_post60\": float(post_med) if np.isfinite(post_med) else np.nan,\n",
    "        \"max_D_0to7\": float(spike) if np.isfinite(spike) else np.nan,\n",
    "    })\n",
    "\n",
    "aligned_df = pd.DataFrame(aligned)\n",
    "summary_df = pd.DataFrame(meta_rows).sort_values(\"start\")\n",
    "\n",
    "print(f\"Included crashes in aggregation: {aligned_df.shape[1]} / {len(crash_events)}\")\n",
    "display(summary_df)\n",
    "\n",
    "\n",
    "# Aggregate statistics (median + IQR)\n",
    "median = aligned_df.median(axis=1, skipna=True)\n",
    "q25 = aligned_df.quantile(0.25, axis=1, interpolation=\"linear\")\n",
    "q75 = aligned_df.quantile(0.75, axis=1, interpolation=\"linear\")\n",
    "\n",
    "# If normalized, also normalize reference band accordingly (not well-defined globally).\n",
    "show_ref = (not NORMALIZE)\n",
    "\n",
    "\n",
    "# Plot 1: Overlay all aligned curves + median/IQR + normal reference band\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "for col in aligned_df.columns:\n",
    "    ax.plot(aligned_df.index, aligned_df[col].values, color=\"steelblue\", alpha=0.18, lw=1.0)\n",
    "\n",
    "ax.plot(median.index, median.values, color=\"navy\", lw=2.5, label=\"Median across crashes\")\n",
    "ax.fill_between(median.index, q25.values, q75.values, color=\"navy\", alpha=0.18, label=\"IQR (25–75%)\")\n",
    "\n",
    "ax.axvline(0, color=\"red\", ls=\"--\", lw=1.5, alpha=0.9, label=\"Crash start (t=0)\")\n",
    "\n",
    "if show_ref:\n",
    "    ax.axhline(norm_med, color=\"black\", ls=\"--\", lw=1.3, alpha=0.9, label=\"Normal median (global)\")\n",
    "    ax.axhspan(norm_q25, norm_q75, color=\"black\", alpha=0.08, label=\"Normal IQR (global)\")\n",
    "\n",
    "ax.set_title(f\"Mahalanobis distance aligned at crash start (±{ALIGN_WINDOW_DAYS} days)\"\n",
    "             + (\" [normalized]\" if NORMALIZE else \"\"))\n",
    "ax.set_xlabel(\"Days relative to crash start\")\n",
    "ax.set_ylabel(\"D (lower = closer to pre-crash centroid)\" + (\" / baseline\" if NORMALIZE else \"\"))\n",
    "ax.legend(loc=\"upper left\")\n",
    "plt.ylim(0, 11)\n",
    "ax.grid(True, alpha=0.25)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D around Black Monday\n",
    "\n",
    "Mahalanobis distance in the period around October 1987.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae345fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic\n",
    "print(\"Min D (pre-2005):\", D.min())\n",
    "print(\"Min D during pre-crash windows:\")\n",
    "\n",
    "mask_pre = build_precrash_mask(D.index, MARKET_EVENTS[\"crashes\"], PRECRASH_L_DAYS)\n",
    "print(D[mask_pre].describe())\n",
    "\n",
    "# Plot D around 1987 crash\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(D.loc[\"1986\":\"1988\"])\n",
    "plt.axvline(pd.Timestamp(\"1987-10-19\"), color=\"r\", ls=\"--\")\n",
    "plt.title(\"Mahalanobis distance around Black Monday\")\n",
    "plt.ylabel(\"D (lower = closer)\")\n",
    "plt.ylim(0, 11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "See code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1758ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Robustness check: \"full-sample geometry\" vs \"pre-2005 geometry\" ---\n",
    "\n",
    "\n",
    "# Settings\n",
    "\n",
    "PRECRASH_L_DAYS = 60          \n",
    "ALIGN_WINDOW_DAYS = 180\n",
    "RESAMPLE = \"D\"\n",
    "MIN_POINTS = 30\n",
    "\n",
    "# Choose which \"pre-geometry\" distance series to compare against\n",
    "D_PRE = D  \n",
    "\n",
    "\n",
    "\n",
    "def aligned_event_matrix(series: pd.Series, crashes, window_days=180, resample=\"D\", min_points=30):\n",
    "    \"\"\"Return aligned_df (rows=relative day, cols=crash name) and per-crash summary.\"\"\"\n",
    "    series = series.dropna().sort_index()\n",
    "    events = [(pd.to_datetime(s), pd.to_datetime(e), name) for s, e, name in crashes]\n",
    "    events = sorted(events, key=lambda x: x[0])\n",
    "\n",
    "    t_grid = pd.timedelta_range(start=-pd.Timedelta(days=window_days),\n",
    "                                end=pd.Timedelta(days=window_days),\n",
    "                                freq=resample)\n",
    "    rel_days = np.array([td.days for td in t_grid], dtype=int)\n",
    "\n",
    "    aligned = {}\n",
    "    meta = []\n",
    "    for start, end, name in events:\n",
    "        w0 = start - pd.Timedelta(days=window_days)\n",
    "        w1 = start + pd.Timedelta(days=window_days)\n",
    "\n",
    "        seg = series.loc[w0:w1].copy()\n",
    "        if resample is not None:\n",
    "            seg = seg.reindex(pd.date_range(w0, w1, freq=resample),\n",
    "                              method=\"nearest\",\n",
    "                              tolerance=pd.Timedelta(days=3))\n",
    "\n",
    "        seg_rel = (seg.index - start).days\n",
    "        seg.index = seg_rel\n",
    "\n",
    "        y = pd.Series(index=rel_days, dtype=float)\n",
    "        y.loc[seg.index.intersection(rel_days)] = seg.values\n",
    "\n",
    "        npts = int(y.notna().sum())\n",
    "        if npts < min_points:\n",
    "            continue\n",
    "\n",
    "        aligned[name] = y\n",
    "        meta.append({\"crash\": name, \"start\": start.date(), \"points_in_window\": npts})\n",
    "\n",
    "    return pd.DataFrame(aligned), pd.DataFrame(meta).sort_values(\"start\")\n",
    "\n",
    "\n",
    "# Fit FULL-sample crash geometry (centroid/cov) from pre-crash windows\n",
    "\n",
    "X_all = Xdf2[[\"z_absH\", \"z_mu\", \"z_l1\"]].dropna().copy()\n",
    "\n",
    "mask_pre_all = build_precrash_mask(X_all.index, MARKET_EVENTS[\"crashes\"], PRECRASH_L_DAYS)\n",
    "X_pre_all = X_all.loc[mask_pre_all]\n",
    "\n",
    "if len(X_pre_all) < 30:\n",
    "    raise ValueError(f\"Too few pre-crash points in full sample: {len(X_pre_all)}\")\n",
    "\n",
    "mu_full = X_pre_all.mean(axis=0).values\n",
    "Sigma_full = np.cov(X_pre_all.values.T)\n",
    "\n",
    "# Shrinkage (same style as your robustness plot) for stability\n",
    "alpha = 0.3\n",
    "Sigma_diag = np.diag(np.diag(Sigma_full))\n",
    "Sigma_full_shrunk = (1 - alpha) * Sigma_full + alpha * Sigma_diag\n",
    "Sigma_inv_full = np.linalg.inv(Sigma_full_shrunk + 1e-8 * np.eye(3))\n",
    "\n",
    "# Compute D under FULL-sample geometry\n",
    "D_FULL = pd.Series(\n",
    "    mahalanobis_distance(X_all.values, mu_full, Sigma_inv_full),\n",
    "    index=X_all.index,\n",
    "    name=f\"D_fullgeom_shrunk_a{alpha}\"\n",
    ")\n",
    "\n",
    "# Build aligned matrices for PRE-geom and FULL-geom distances\n",
    "\n",
    "aligned_pre, meta_pre = aligned_event_matrix(D_PRE, MARKET_EVENTS[\"crashes\"],\n",
    "                                            window_days=ALIGN_WINDOW_DAYS, resample=RESAMPLE,\n",
    "                                            min_points=MIN_POINTS)\n",
    "aligned_full, meta_full = aligned_event_matrix(D_FULL, MARKET_EVENTS[\"crashes\"],\n",
    "                                              window_days=ALIGN_WINDOW_DAYS, resample=RESAMPLE,\n",
    "                                              min_points=MIN_POINTS)\n",
    "\n",
    "# Use only crashes present in BOTH matrices\n",
    "common = sorted(set(aligned_pre.columns).intersection(aligned_full.columns))\n",
    "aligned_pre = aligned_pre[common]\n",
    "aligned_full = aligned_full[common]\n",
    "\n",
    "med_pre = aligned_pre.median(axis=1, skipna=True)\n",
    "q25_pre = aligned_pre.quantile(0.25, axis=1)\n",
    "q75_pre = aligned_pre.quantile(0.75, axis=1)\n",
    "\n",
    "med_full = aligned_full.median(axis=1, skipna=True)\n",
    "q25_full = aligned_full.quantile(0.25, axis=1)\n",
    "q75_full = aligned_full.quantile(0.75, axis=1)\n",
    "\n",
    "\n",
    "# Plot: median + IQR for both geometries\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "# PRE-2005 geometry (baseline)\n",
    "ax.plot(med_pre.index, med_pre.values, color=\"navy\", lw=2.5, label=\"Median D (pre-2005 geometry)\")\n",
    "ax.fill_between(med_pre.index, q25_pre.values, q75_pre.values, color=\"navy\", alpha=0.15, label=\"IQR pre-2005\")\n",
    "\n",
    "# FULL-sample geometry (robustness)\n",
    "ax.plot(med_full.index, med_full.values, color=\"darkgreen\", lw=2.5, label=\"Median D (full-sample geometry)\")\n",
    "ax.fill_between(med_full.index, q25_full.values, q75_full.values, color=\"darkgreen\", alpha=0.12, label=\"IQR full-sample\")\n",
    "\n",
    "ax.axvline(0, color=\"red\", ls=\"--\", lw=1.5, label=\"Crash start (t=0)\")\n",
    "ax.set_title(f\"Robustness: aligned Mahalanobis distance (±{ALIGN_WINDOW_DAYS}d), common crashes={len(common)}\")\n",
    "ax.set_xlabel(\"Days relative to crash start\")\n",
    "ax.set_ylabel(\"D (lower = closer to pre-crash centroid)\")\n",
    "ax.grid(True, alpha=0.25)\n",
    "ax.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
